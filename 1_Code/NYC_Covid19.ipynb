{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"nyc\"><img src = \"https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2013/09/nyc.jpg\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>COVID-19 in New York City</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<font size = 3>\n",
    "<li>1.<a href=\"#item1\">Download New York City Datasets</a></li>\n",
    "<li>2.<a href=\"#item2\">Download Foursquare API Datasets</a></li>\n",
    "<li>3.<a href=\"#item3\">Analyze Each Neighborhood</a></li>\n",
    "<li>4.<a href=\"#item4\">Cluster Neighborhoods</a></li>\n",
    "<li>5.<a href=\"#item5\">Examine Clusters</a>    </li>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get the data and start exploring it, let's download all the dependencies that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library to handle data in a vectorized manner\n",
    "import numpy as np \n",
    "\n",
    "# library for data analsysis\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# library to handle JSON files\n",
    "import json \n",
    "\n",
    "# convert an address into latitude and longitude values\n",
    "from geopy.geocoders import Nominatim \n",
    "\n",
    "# library to handle requests\n",
    "import requests \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Unverified HTTPS request')\n",
    "\n",
    "# tranform JSON file into a pandas dataframe\n",
    "from pandas.io.json import json_normalize \n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# map rendering library\n",
    "import folium \n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foursquare_API = False\n",
    "neighbourhood_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download New York City datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load New-York City Zip Codes / Neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :\n",
    "- https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm\n",
    "- https://github.com/MacHu-GWU/uszipcode-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighborhood = pd.read_excel('zipcodeNYC.xlsx')\n",
    "df_neighborhood.columns = [ii.lower() for ii in df_neighborhood.columns]\n",
    "df_neighborhood = df_neighborhood.rename(columns={'zipcode':'zip_code'})\n",
    "# Strip spaces in zipcode string\n",
    "df_neighborhood['zip_code']=df_neighborhood['zip_code'].astype('str').str.strip()\n",
    "print(df_neighborhood.shape)\n",
    "df_neighborhood.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load New-York City Demographic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : http://www.city-data.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "df_neighborhood['density']=np.nan\n",
    "df_neighborhood['median_house_value']=np.nan\n",
    "df_neighborhood['median_age']=np.nan\n",
    "df_neighborhood['avg_household_size']=np.nan\n",
    "df_neighborhood['avg_agi']=np.nan\n",
    "df_neighborhood['avg_wage']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if neighbourhood_data:\n",
    "    for ii in range(df_neighborhood.shape[0]):\n",
    "        \n",
    "        # Zip Code\n",
    "        zipcode=str(df_neighborhood['zip_code'].iloc[ii])\n",
    "        \n",
    "        \n",
    "        print('-----')\n",
    "        print(zipcode)\n",
    "            \n",
    "        # Parse HTML data from website \n",
    "        data = pd.DataFrame(pd.read_html(r'http://www.city-data.com/zips/'+zipcode+'.html'))\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            density = int(str(data.iloc[8,0][0]).split('density: ')[1].split(' people')[0].replace(',',''))\n",
    "            median_house_value = int(str(data.iloc[13,0][1][0]).split('$')[1].replace(',',''))\n",
    "            median_age = float(str(data.iloc[14,0][1][0]).split(' years')[0])\n",
    "            avg_household_size = float(str(data.iloc[15,0][1][0]).split(' people')[0])\n",
    "            avg_agi = int(str(data.iloc[16,0][1][0]).replace(',','').replace('$',''))\n",
    "            avg_wage = int(str(data.iloc[17,0][1][0]).replace(',','').replace('$',''))\n",
    "\n",
    "            #Save data in the dataframe\n",
    "            df_neighborhood['density'].iloc[ii]=density\n",
    "            df_neighborhood['median_house_value'].iloc[ii]=median_house_value\n",
    "            df_neighborhood['median_age'].iloc[ii]=median_age\n",
    "            df_neighborhood['avg_household_size'].iloc[ii]=avg_household_size\n",
    "            df_neighborhood['avg_agi'].iloc[ii]=avg_agi\n",
    "            df_neighborhood['avg_wage'].iloc[ii]=avg_wage\n",
    "            \n",
    "       \n",
    "        except :\n",
    "            print('--ERROR--')\n",
    "         \n",
    "    \n",
    "    # Export Data\n",
    "    df_neighborhood.to_csv('data_neighborhood.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighborhood = pd.read_csv('data_neighborhood.csv',sep=';', index_col=0)\n",
    "df_neighborhood['zip_code']=df_neighborhood['zip_code'].astype('str').str.strip()\n",
    "df_neighborhood.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighborhood['total_population']=df_neighborhood['density']*df_neighborhood['landsqmi']\n",
    "df_neighborhood['total_population']=df_neighborhood['total_population'].astype('float64')\n",
    "df_neighborhood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows containg blank cells\n",
    "df_neighborhood=df_neighborhood.dropna(how ='any')\n",
    "df_neighborhood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load New-York City COVID-19 cases - by Zip Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source : Github - https://github.com/nychealth/coronavirus-data\n",
    "- Updated : every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_github_data='https://raw.githubusercontent.com/nychealth/coronavirus-data/master/tests-by-zcta.csv'\n",
    "# Load data\n",
    "df_covid19=pd.read_csv(url_github_data, index_col=0).reset_index()\n",
    "# Rename columns\n",
    "df_covid19=df_covid19.rename(columns={'MODZCTA':'zip_code','Positive':'nb_positive_covid19'})\n",
    "# Drop unused columns and null zip codes\n",
    "df_covid19=df_covid19[['zip_code','nb_positive_covid19']].dropna(subset=['zip_code'])\n",
    "# Strip spaces in zipcode string\n",
    "df_covid19['zip_code']=df_covid19['zip_code'].astype('int64').astype('str').str.strip()\n",
    "print(df_covid19.shape)\n",
    "df_covid19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge with COVID-19 dataset, ignoring Neighborhoods without data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with demographic data - Inner join\n",
    "df_data = pd.merge(df_neighborhood,df_covid19,on='zip_code',how='inner')\n",
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate %_pop_positive_covid19\n",
    "df_data['perc_pop_positive_covid19'] = df_data['nb_positive_covid19']/df_data['total_population']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Download New-York City map with Zip Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source : https://geo.nyu.edu/catalog/harvard-tg00nyzcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nyzcta-geojson.json') as json_data:\n",
    "    newyork_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Vizualize the number of people tested positive for COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use geopy library to get the latitude and longitude values of New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent <em>ny_explorer</em>, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "address = 'New York City, NY'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"ny_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of New York City are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a NYC map\n",
    "NYC_map = folium.Map(location=[latitude, longitude], zoom_start=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate choropleth map using the total immigration of each country to Canada from 1980 to 2013\n",
    "folium.Choropleth(\n",
    "    geo_data=newyork_data,\n",
    "    data=df_data,\n",
    "    columns=['zip_code', 'perc_pop_positive_covid19'],\n",
    "    key_on='feature.properties.ZCTA',\n",
    "    fill_color='YlOrRd', \n",
    "    nan_fill_color ='#e7dddd',\n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='People tested positive for COVID-19 in New-York City'\n",
    ").add_to(NYC_map)\n",
    "\n",
    "# display map\n",
    "display(NYC_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download Foursquare API datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Foursquare Credentials and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if foursquare_API :\n",
    "    CLIENT_ID = 'AKC0LB0R50HQDF5ZJNKXKADAVJDRVJLKSOOOUT3GLWMIVRHM' # your Foursquare ID\n",
    "    CLIENT_SECRET = '4JYPUHE2U5TZBMO0ULPBP22A53NFHO32MQFSCVKN0AYECMM2' # your Foursquare Secret\n",
    "    VERSION = '20200301'\n",
    "\n",
    "    print('Your credentails:')\n",
    "    print('CLIENT_ID: ' + CLIENT_ID)\n",
    "    print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for specific venue categories - 500m\n",
    "> `https://api.foursquare.com/v2/venues/`**search**`?client_id=`**CLIENT_ID**`&client_secret=`**CLIENT_SECRET**`&ll=`**LATITUDE**`,`**LONGITUDE**`&v=`**VERSION**`&query=`**QUERY**`&radius=`**RADIUS**`&limit=`**LIMIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categories** that will be counted :\n",
    "- **Medical center** :  '4bf58dd8d48988d104941735,'\n",
    "- **Arts & Entertainment** : '4d4b7104d754a06370d81259'\n",
    "- **Nightlife spot** : '4d4b7105d754a06376d81259'\n",
    "- **Outdoors & Recreation** : '4d4b7105d754a06377d81259'\n",
    "- **Shop & Service** : '4d4b7105d754a06378d81259'\n",
    "- **Food **:'4d4b7105d754a06374d81259'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if foursquare_API :\n",
    "    category_id = ['4bf58dd8d48988d104941735','4d4b7104d754a06370d81259','4d4b7105d754a06376d81259','4d4b7105d754a06377d81259',\\\n",
    "                 '4d4b7105d754a06378d81259','4d4b7105d754a06374d81259']\n",
    "    category_name =  ['medical_center','arts_entertainment','nightlife_spot','outdoors_recreation','shop_service','food']\n",
    "\n",
    "    # Initialize\n",
    "    df_foursquare_API = pd.DataFrame()\n",
    "    df_foursquare_API=df_data.copy()\n",
    "    for label in category_name:\n",
    "        df_foursquare_API[label]=np.nan\n",
    "    df_foursquare_API.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foursquare_API_data(df, categoryId, categoryName, zipcode):\n",
    "\n",
    "    # Parameters\n",
    "    radius = 500\n",
    "    LIMIT = 100\n",
    "    # Serach for a Specific Category\n",
    "    search_query = categoryId\n",
    "    # Get zip_code latitude/longitude\n",
    "    latitude = df.loc[zipcode,'latitude']\n",
    "    longitude = df.loc[zipcode,'longitude']\n",
    "\n",
    "    # Build the URL\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&categoryId={}&radius={}&limit={}'\\\n",
    "    .format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n",
    "\n",
    "    # Send the GET request\n",
    "    results = requests.get(url,verify=False).json()\n",
    "\n",
    "    # assign relevant part of JSON to venues\n",
    "    venues = results['response']['venues']\n",
    "\n",
    "    # tranform venues into a dataframe\n",
    "    dataframe = pd.json_normalize(venues)\n",
    "\n",
    "    df[categoryName].loc[zipcode] = dataframe.shape[0]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You need an upgraded Foursquare account to run this function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if foursquare_API :\n",
    "    for ii in range(len(category_id)):\n",
    "        for jj in range(df_data.shape[0]):\n",
    "            try :\n",
    "                print('-----')\n",
    "                print('ii :',ii,' jj : ',jj)\n",
    "                df_foursquare_API = get_foursquare_API_data(df_foursquare_API, category_id[ii], category_name[ii], jj)\n",
    "                if foursquare_API :\n",
    "                    df_foursquare_API.to_csv('data_nyc.csv',sep=';')\n",
    "                print(df_foursquare_API.iloc[jj,:])\n",
    "            except :\n",
    "                print(ii)\n",
    "                print(jj)\n",
    "                break\n",
    "\n",
    "    df_foursquare_API.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_data = pd.read_csv('data_nyc.csv',sep=';', index_col=0)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n",
    "        len(df_data['borough'].unique()),\n",
    "        df_data.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[list(df_data.columns)[5:]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only keep useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep useful columns\n",
    "list_cols = list(df_data.columns)\n",
    "list_cols = list_cols[0:5]+list_cols[6:10]+[list_cols[11]]+list_cols[14:17]+list_cols[18:20]\n",
    "\n",
    "# Create dataframe used for machine learning\n",
    "df_ml = df_data.copy(deep=True)[list_cols]    \n",
    "\n",
    "\n",
    "# Rename columns\n",
    "for ii in range(11,len(list_cols)):\n",
    "    list_cols[ii]='nb_venue_'+list_cols[ii]\n",
    "df_ml.columns=list_cols\n",
    "df_ml.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = df_ml['avg_wage']      # year on x-axis\n",
    "y = df_ml['perc_pop_positive_covid19']     # total on y-axis\n",
    "fit = np.polyfit(x, y, deg=1)\n",
    "\n",
    "fit\n",
    "\n",
    "df_ml.plot(kind='scatter', y='perc_pop_positive_covid19', x='avg_wage', ylim=[0,0.035],figsize=(10, 6), color='darkblue')\n",
    "\n",
    "plt.title('% of Population tested Positive to COVID-19 versus Average annual wage [$]')\n",
    "plt.xlabel('Average annual wage [$]')\n",
    "plt.ylabel('% of Population tested Positive to COVID-19')\n",
    "\n",
    "# plot line of best fit\n",
    "plt.plot(x, fit[0] * x + fit[1], color='red') # recall that x is the Years\n",
    "plt.annotate('y={0:.10f} x + {1:.4f}'.format(fit[0], fit[1]), xy=(150000,0.03))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print out the line of best fit\n",
    "#print('No. Immigrants = {0:.0f} * Year + {1:.0f}'.format(fit[0], fit[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cluster Neighbourhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing over the standard deviation\n",
    "Now let's normalize the dataset. Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. We use __StandardScaler()__ to normalize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_ml.values[:,5:]\n",
    "X = np.nan_to_num(X)\n",
    "Clus_dataSet = StandardScaler().fit_transform(X)\n",
    "Clus_dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chose the right number of clusters - Elbow method and Silhouette score\n",
    "\n",
    "A higher Silhouette Coefficient score relates to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores: `\n",
    "\n",
    "- a: The mean distance between a sample and all other points in the same class.\n",
    "- b: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "The Silhouette Coefficient is for a single sample is then given as:\n",
    "s=b-a/max(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "sse={}\n",
    "sil_coeff_x=[]\n",
    "sil_coeff_y=[]\n",
    "\n",
    "for k in range(1, 10):#\n",
    "    kmeans = KMeans( init = \"k-means++\",n_clusters = k, n_init = 20, max_iter = 1000).fit(Clus_dataSet)\n",
    "    label = kmeans.labels_\n",
    "    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "    if k>=2:\n",
    "        sil_coeff_y.append(silhouette_score(Clus_dataSet, label, metric='euclidean')) # Silhouette Coefficient \n",
    "        sil_coeff_x.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.title(\"Elbow Criterion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sil_coeff_x, sil_coeff_y)\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.title(\"Silhouette Coefficient Criterion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_clusters = 4\n",
    "kmeans = KMeans( init = \"k-means++\",n_clusters = k_clusters, n_init = 20, max_iter = 1000).fit(X)\n",
    "df_ml['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.groupby('cluster').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color scheme for the clusters\n",
    "x = np.arange(k_clusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(k_clusters)]\n",
    "rainbow = ['#3554ee','#42ee27','#a96f56','#8c26f6']\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(df_ml['latitude'], df_ml['longitude'], df_ml['neighborhood'], df_ml['cluster']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(NYC_map)\n",
    "       \n",
    "NYC_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating train and test dataset\n",
    "Train/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. \n",
    "This will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the data. It is more realistic for real world problems.\n",
    "\n",
    "This means that we know the outcome of each data point in this dataset, making it great to test with! And since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it’s truly an out-of-sample testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df_ml)) < 0.8\n",
    "train = df_ml[msk]\n",
    "test = df_ml[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "independant_variable=list(df_ml.columns)[5:]\n",
    "independant_variable.remove('perc_pop_positive_covid19')\n",
    "independant_variable.remove('cluster')\n",
    "independant_variable.remove('median_age')\n",
    "#independant_variable.remove('density')\n",
    "independant_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependant_variable=['perc_pop_positive_covid19']\n",
    "dependant_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize linear model\n",
    "regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "# Initialize normalized features\n",
    "x_train = np.asanyarray(train[independant_variable])\n",
    "x_train_scale = StandardScaler().fit_transform(x_train)\n",
    "\n",
    "y_train = np.asanyarray(train[dependant_variable])\n",
    "y_train_scale = StandardScaler().fit_transform(y_train)\n",
    "\n",
    "x_test = np.asanyarray(test[independant_variable])\n",
    "x_test_scale = StandardScaler().fit_transform(x_test)\n",
    "\n",
    "y_test = np.asanyarray(test[dependant_variable])\n",
    "y_test_scale = StandardScaler().fit_transform(y_test)\n",
    "\n",
    "# Fit model\n",
    "regr.fit (x_train_scale, y_train_scale)\n",
    "\n",
    "# The coefficients\n",
    "print ('Coefficients: ', regr.coef_)\n",
    "\n",
    "# Predict test values\n",
    "y_hat_scale= regr.predict(x_test_scale)\n",
    "print(\"Intercept: %.2f\" % regr.intercept_ )\n",
    "print(\"Residual sum of squares (RSS) : %.2f\"\n",
    "      % np.mean((y_hat_scale - y_test_scale) ** 2))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % regr.score(x_test_scale,y_test_scale ))\n",
    "\n",
    "print(\"R2-score: %.2f\" % r2_score(y_hat_scale , y_test_scale) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independant_variable='avg_wage'\n",
    "dependant_variable='perc_pop_positive_covid19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fkt(a, b, x):\n",
    "    '''\n",
    "    Returns array of a horizontal mirrored normalized sigmoid function\n",
    "    Function parameters a = center; b = width\n",
    "    '''\n",
    "    s= 1/(1+np.exp(b*(x-a)))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "# Initialize linear model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Min/Max\n",
    "max_x = max([max(train[independant_variable]),max(test[independant_variable])])\n",
    "max_y = max([max(train[dependant_variable]),max(test[dependant_variable])])\n",
    "\n",
    "# Initialize normalized features\n",
    "x_train = np.asanyarray(train[independant_variable],dtype=float)/max_x\n",
    "\n",
    "y_train = np.asanyarray(train[dependant_variable],dtype=float)/max_y\n",
    "\n",
    "x_test = np.asanyarray(test[independant_variable],dtype=float)/max_x\n",
    "\n",
    "y_test = np.asanyarray(test[dependant_variable],dtype=float)/max_y\n",
    "\n",
    "\n",
    "# build the model using train set\n",
    "popt, pcov = curve_fit(sigmoid_fkt, x_train, y_train, maxfev=5000)\n",
    "\n",
    "# predict using test set\n",
    "y_hat = sigmoid_fkt(x_test, *popt)\n",
    "\n",
    "# evaluation\n",
    "print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\n",
    "print(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\n",
    "print(\"R2-score: %.2f\" % r2_score(y_hat , y_test) )\n",
    "\n",
    "#Plot\n",
    "plt.scatter(x_train, y_train,  color='blue')\n",
    "plt.scatter(x_test, y_test,  color='green')\n",
    "plt.scatter(x_test, y_hat,  color='red')\n",
    "XX = np.arange(0,1,0.001)\n",
    "yy = sigmoid_fkt(XX, *popt)\n",
    "plt.plot(XX, yy, '-r' )\n",
    "plt.ylabel('Infected population')\n",
    "plt.xlabel('Average annual wage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
